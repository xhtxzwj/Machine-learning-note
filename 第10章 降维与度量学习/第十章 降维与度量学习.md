<center><font size=13>第十章 降维与度量学习</font></center>
# 10.0 本章线性代数基础知识

本部分内容参考于<线性代数(第五版)>以及"彬彬有礼的专栏", 博客地址: https://blog.csdn.net/jbb0523

## 10.0.1 符号说明

向量元素之间**分号 “;” 表示列元素分隔符**,  如 $\boldsymbol{\alpha}=\left(a_{1} ; a_{2} ; \ldots ; a_{i} ; \ldots ; a_{m}\right)$ 表示 $m \times 1$ 的**列向量**;  

而**逗号 "," 表示行元素分隔符**,  如 $\boldsymbol{\alpha}=\left(a_{1}, a_{2}, \ldots, a_{i}, \dots, a_{m}\right)$ 表示 $1 \times m$ 的**行向量**.



## 10.0.2 矩阵与单位阵、向量的乘法

1.  **矩阵左乘对角阵**相当于**矩阵每行**乘以**对应对角阵**的**对角线元素**,  具体如:

$$
\left[\begin{array}{ccc}{\lambda_{1}} & {} & {} \\ {} & {\lambda_{2}} & {} \\ {} & {} & {\lambda_{3}}\end{array}\right]\left[\begin{array}{lll}{x_{11}} & {x_{12}} & {x_{13}} \\ {x_{21}} & {x_{22}} & {x_{23}} \\ {x_{31}} & {x_{32}} & {x_{33}}\end{array}\right]=\left[\begin{array}{lll}{\lambda_{1} x_{11}} & {\lambda_{1} x_{12}} & {\lambda_{1} x_{13}} \\ {\lambda_{2} x_{21}} & {\lambda_{2} x_{22}} & {\lambda_{2} x_{23}} \\ {\lambda_{3} x_{31}} & {\lambda_{3} x_{32}} & {\lambda_{3} x_{33}}\end{array}\right]
$$

2. **矩阵右乘对角阵**相当于**矩阵每列**乘以**对应对角阵**的**对角线元素**,  具体如:

$$
\left[\begin{array}{lll}{x_{11}} & {x_{12}} & {x_{13}} \\ {x_{21}} & {x_{22}} & {x_{23}} \\ {x_{31}} & {x_{32}} & {x_{33}}\end{array}\right]\left[\begin{array}{ccc}{\lambda_{1}} & {} & {} \\ {} & {\lambda_{2}} & {} \\ {} & {} & {\lambda_{3}}\end{array}\right]=\left[\begin{array}{ccc}{\lambda_{1} x_{11}} & {\lambda_{2} x_{12}} & {\lambda_{3} x_{13}} \\ {\lambda_{1} x_{21}} & {\lambda_{2} x_{22}} & {\lambda_{3} x_{23}} \\ {\lambda_{1} x_{31}} & {\lambda_{2} x_{32}} & {\lambda_{3} x_{33}}\end{array}\right]
$$

3. **矩阵左乘行向量**相当于**矩阵每行**乘以**对应行向量**的**元素之和**, 具体如:
   $$
   \left[\begin{array}{lll}{\lambda_{1}} & {\lambda_{2}} & {\lambda_{3}}\end{array}\right]\left[\begin{array}{lll}{x_{11}} & {x_{12}} & {x_{13}} \\ {x_{21}} & {x_{22}} & {x_{23}} \\ {x_{31}} & {x_{32}} & {x_{33}}\end{array}\right]
   $$

   $$
   =\lambda_{1}\left[\begin{array}{lll}{x_{11}} & {x_{12}} & {x_{13}}\end{array}\right]+\lambda_{2}\left[\begin{array}{ccc}{x_{21}} & {x_{22}} & {x_{23}}\end{array}\right]+\lambda_{3}\left[\begin{array}{ccc}{x_{31}} & {x_{32}} & {x_{33}}\end{array}\right]
   $$

   

$$
=\left(\lambda_{1} x_{11}+\lambda_{2} x_{21}+\lambda_{3} x_{31}, \lambda_{1} x_{12}+\lambda_{2} x_{22}+\lambda_{3} x_{32}, \lambda_{1} x_{13}+\lambda_{2} x_{23}+\lambda_{3} x_{33}\right)
$$



4. **矩阵右乘列向量**相当于**矩阵每列**乘以**对应列向量**的**元素之和**,  具体如:
   $$
   \left[\begin{array}{lll}{x_{11}} & {x_{12}} & {x_{13}} \\ {x_{21}} & {x_{22}} & {x_{23}} \\ {x_{31}} & {x_{32}} & {x_{33}}\end{array}\right]\left[\begin{array}{l}{\lambda_{1}} \\ {\lambda_{2}} \\ {\lambda_{3}}\end{array}\right]
   $$

   $$
   \begin{array}{l}{=\lambda_{1}\left[\begin{array}{l}{x_{11}} \\ {x_{21}} \\ {x_{31}}\end{array}\right]+\lambda_{2}\left[\begin{array}{c}{x_{12}} \\ {x_{22}} \\ {x_{32}}\end{array}\right]+\lambda_{3}\left[\begin{array}{c}{x_{13}} \\ {x_{23}} \\ {x_{33}}\end{array}\right]=\sum_{i=1}^{3}\left(\lambda_{i}\left[\begin{array}{l}{x_{1 i}} \\ {x_{2 i}} \\ {x_{3 i}}\end{array}\right]\right)} \\ {=\left(\lambda_{1} x_{11}+\lambda_{2} x_{12}+\lambda_{3} x_{13} ; \lambda_{1} x_{21}+\lambda_{2} x_{22}+\lambda_{3} x_{23} ; \lambda_{1} x_{31}+\lambda_{2} x_{32}+\lambda_{3} x_{33}\right)}\end{array}
   $$

   

**小结**:  **左乘**是对矩阵的**行操作**,  而**右乘**则是对矩阵的**列操作**.



> ------
>
> **注1**:  **什么是对角阵**
>
> 对角阵也即是对角矩阵,  **它是一个主对角线之外的元素都为 $0$ 的矩阵**. 对角线上的元素可以为 $0$ 或其他值(不能全为 $0$ ).  令 $a_{i,j}$ 表示矩阵的元素,  则对角矩阵中满足 $ a_{i, j}=0 \text { if } i \neq j \quad \forall i, j \in\{1,2, \ldots, n\} $ . 
>
> 常记为:  $A=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \cdots, \lambda_{n}\right)$ , 其中 $\lambda_{1}, \lambda_{2}, \cdots, \lambda_{n}$ 为主对角线上的元素. 
>
> 
>
> 同时**几个特殊的对角矩阵**:
>
> - 对角线上元素相等的对角矩阵称为**数量矩阵**；
> - 对角线上元素全为 $1$ 的对角矩阵称为**单位矩阵**

------



## 10.0.3 矩阵的 $F$ 范数与迹

1. **矩阵的 $F$ 范数**

   对于矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ ,  其 $F$ 范数 $\|\mathbf{A}\|_{F}$ 定义为:

   
   $$
   \|\mathbf{A}\|_{F}=\sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n}\left|a_{i j}\right|^{2}}
   $$
   其中, $a_{ij}$ 为矩阵 $\mathbf{A}$ 第 $i$ 行第 $j$ 列元素, 即
   $$
   \mathbf{A}=\left[\begin{array}{cccccc}{a_{11}} & {a_{12}} & {\cdots} & {a_{1 j}} & {\cdots} & {a_{1 n}} \\ {a_{21}} & {a_{22}} & {\cdots} & {a_{2 j}} & {\cdots} & {a_{2 n}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {a_{i 1}} & {a_{i 2}} & {\cdots} & {a_{i j}} & {\cdots} & {a_{i n}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {a_{m 1}} & {a_{m 2}} & {\cdots} & {a_{m j}} & {\cdots} & {a_{m n}}\end{array}\right]
   $$
   
2. **列向量和行向量**表示的**矩阵的 $F$ 范数**

   若 $\mathbf{A}=\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \ldots, \boldsymbol{\alpha}_{j}, \ldots, \boldsymbol{\alpha}_{n}\right)$ ,  其中 $\boldsymbol{\alpha}_{j}=\left(a_{1 j} ; a_{2 j} ; \ldots ; a_{i j} ; \ldots ; a_{m j}\right)$ 为其列向量,  $\mathbf{A} \in \mathbb{R}^{m \times n}$,  $\boldsymbol{\alpha}_{j} \in \mathbb{R}^{m \times 1}$ ,  <font color=red>则 $\|\mathbf{A}\|_{F}^{2}=\sum_{j=1}^{n}\left\|\boldsymbol{\alpha}_{j}\right\|_{2}^{2}$;  且有 $\|\boldsymbol{\alpha}_{j}\|_{2}^{2}=\boldsymbol{\alpha}_{j}^{T}\boldsymbol{\alpha}_{j}$ </font>

   同理,  若 $\mathbf{A}=\left(\boldsymbol{\beta}_{1} ; \boldsymbol{\beta}_{2} ; \ldots ; \boldsymbol{\beta}_{i} ; \ldots ; \boldsymbol{\beta}_{m}\right)$ ,  其中 $\boldsymbol{\beta}_{i}=\left(a_{i 1}, a_{i 2}, \ldots, a_{i j}, \ldots, a_{i n}\right)$ 为其行向量, $\mathbf{A} \in \mathbb{R}^{m \times n}$ , 

   $\boldsymbol{\beta}_{i} \in \mathbb{R}^{1 \times n}$ ,  <font color=red>则 $\|\mathbf{A}\|_{F}^{2}=\sum_{i=1}^{m}\left\|\boldsymbol{\beta}_{i}\right\|_{2}^{2}$ .且有 $\|\boldsymbol{\beta}_{i}\|_{2}^{2}=\boldsymbol{\beta}_{i}\boldsymbol{\beta}_{i}^{T}$ </font>

   > ------
   >
   > **注2:** 证明 $\|\mathbf{A}\|_{F}^{2}=\sum_{j=1}^{n}\left\|\boldsymbol{\alpha}_{j}\right\|_{2}^{2}$;
   >
   > 因为 $\left\|\boldsymbol{\alpha}_{j}\right\|_{2}^{2}=\sum_{i=1}^{m}\left|a_{i j}\right|^{2}$ ,  且  $\|\mathbf{A}\|^{2}_{F}=\sum_{i=1}^{m} \sum_{j=1}^{n}\left|a_{i j}\right|^{2}$ , 同理, 可证 $\|\mathbf{A}\|_{F}^{2}=\sum_{i=1}^{m}\left\|\boldsymbol{\beta}_{i}\right\|_{2}^{2}$ .



3. **矩阵的迹**

   在线性代数中,  一个 $n×n$ 矩阵 $\boldsymbol A$ 的**主对角线** (从左上方至右下方的对角线) 上**各个元素的总和**被称为**矩阵 $\boldsymbol A$ 的迹**(或迹数),  一般记作 $\operatorname{tr}(\mathbf{A})$ 

   

4. **方阵的特征值与迹之间的关系**

   设 $n$ 阶矩阵 $\boldsymbol{A}=\left(a_{i j}\right)$ 的特征值为 $\lambda_{1}, \lambda_{2}, \cdots, \lambda_{n}$ ,  则有:

   (1) **矩阵 $\boldsymbol{A}$ 的特征值之和等于 $\boldsymbol{A}$ 的迹**, 也就是主对角线元素的总和. 即:

   
   $$
   \color{red} \lambda_{1}+\lambda_{2}+\cdots+\lambda_{n}=\operatorname{tr}(\mathbf{A})=a_{11}+a_{22}+\cdots+a_{m n}
   $$
   (2)**矩阵 $\boldsymbol{A}$ 的特征值的积等于 $\boldsymbol{A}$ 的行列式**, 即:
   $$
   \color{red} \lambda_{1} \lambda_{2} \cdots \lambda_{n}=|A|
   $$

5. 矩阵的**范数与迹、特征值**之间的关系

   若 $\lambda_{j}$ 表示 $n$ 阶方阵 $\mathbf{A}^{\top} \mathbf{A}$ 的第 $j$ 个特征值, $\operatorname{tr}\left(\mathbf{A}^{\top} \mathbf{A}\right)$ 是 $\mathbf{A}^{\top} \mathbf{A}$ 的迹;  $\lambda_{i}$ 表示 $m$ 阶方阵 $\mathbf{A} \mathbf{A}^{\top}$ 的第 $i$ 个特征值,  $\operatorname{tr}\left(\mathbf{A} \mathbf{A}^{\top}\right)$ 是 $\mathbf{A} \mathbf{A}^{\top}$ 的迹,  那么有:
   $$
   \color{red} \begin{aligned}\|\mathbf{A}\|_{F}^{2} &=\operatorname{tr}\left(\mathbf{A}^{\top} \mathbf{A}\right)=\sum_{j=1}^{n} \lambda_{j}\\ &=\operatorname{tr}\left(\mathbf{A} \mathbf{A}^{\top}\right)=\sum_{i=1}^{m} \lambda_{i} \end{aligned}
   $$

   > ------
   >
   > **注3: 证明上式**
   >
   > 证明 $\|\mathbf{A}\|_{F}^{2}=\operatorname{tr}\left(\mathbf{A}^{\top} \mathbf{A}\right)$ :
   >
   > 令 $\mathbf{B}=\mathbf{A}^{\top} \mathbf{A} \in \mathbb{R}^{n \times n}$ ,  $b_{ij}$ 表示 $\mathbf{B}$ 第 $i$ 行第 $j$ 列元素, 易知 $\operatorname{tr}(\mathbf{B})=\sum_{j=1}^{n} b_{j j}$ , 且有
   > $$
   > \mathbf{B}=\mathbf{A}^{\top} \mathbf{A}=\left[\begin{array}{cccccc}{a_{11}} & {a_{21}} & {\cdots} & {a_{i 1}} & {\cdots} & {a_{m 1}} \\ {a_{12}} & {a_{22}} & {\cdots} & {a_{i 2}} & {\cdots} & {a_{m 2}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {\color{red}a_{1 j}} & {\color{red}a_{2 j}} & {\cdots} & {\color{red}a_{i j}} & {\cdots} & {\color{red}a_{m j}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {a_{1 n}} & {a_{2 n}} & {\cdots} & {a_{i n}} & {\cdots} & {a_{m n}}\end{array}\right] \left[\begin{array}{cccccc}{a_{11}} & {a_{12}} & {\cdots} & {\color{red} a_{1 j}} & {\cdots} & {a_{1 n}} \\ {a_{21}} & {a_{22}} & {\cdots} & {\color{red} a_{2 j}} & {\cdots} & {a_{2 n}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {a_{i 1}} & {a_{i 2}} & {\cdots} & {\color{red} a_{i j}} & {\cdots} & {a_{i n}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {a_{m 1}} & {a_{m 2}} & {\cdots} & {\color{red} a_{m j}} & {\cdots} & {a_{m n}}\end{array}\right]
   > $$
   > 由行列式的计算可知, $b_{ij}$ 等于 $\mathbf{A}^{\top}$ 的第 $j$ 行与 $\mathbf{A}$ 的第 $j$ 列的内积,  也就是上面的红色元素对应积的和. 则
   > $$
   > \operatorname{tr}(\mathbf{B})=\sum_{j=1}^{n} b_{j j}=\sum_{j=1}^{n}\left(\sum_{i=1}^{m}\left|a_{i j}\right|^{2}\right)=\sum_{i=1}^{m} \sum_{j=1}^{n}\left|a_{i j}\right|^{2}=\|\mathbf{A}\|_{F}^{2}
   > $$
   > 因为 $\operatorname{tr}(\mathbf{B})=\operatorname{tr}(\mathbf{A}^{\top} \mathbf{A})$ ,  所以 $\|\mathbf{A}\|_{F}^{2}=\operatorname{tr}\left(\mathbf{A}^{\top} \mathbf{A}\right)$ ,  
   >
   > 同时由方阵与特征值的关系可得出结论
   > $$
   > \|\mathbf{A}\|_{F}^{2}=\operatorname{tr}\left(\mathbf{A}^{\top} \mathbf{A}\right)=\sum_{j=1}^{n} \lambda_{j}
   > $$
   > 同理,  可知: $\|\mathbf{A}\|_{F}^{2}=\operatorname{tr}\left(\mathbf{A} \mathbf{A}^{\top}\right)=\sum_{i=1}^{m} \lambda_{i}$ , 得证.

   ------

   



# 10.1 $k$ 近邻学习

## 10.1.1 $k$ 近邻学习的概念

$k$ **近邻** (k-Nearest Neighbor,  简称 kNN) 学习是一种常用的**监督学习**方法.

工作机制:  给定测试样本,  基于**某种距离度量**找出训练集中与其**最靠近的 $k$ 个训练样本**,  然后基于这 **$k$ 个"邻居 "**的信息来进行**预测**.

**如何基于 $k$ 个"邻居"的信息进行预测:**   

- **在分类任务中:**  常用**"投票法"**,  即选择这 $k$ 个样本中出现**最多的类别标记**作为**预测结果**. 

- **在回归任务中:**  常用**"平均法"**,  即将这 $k$ 个样本的**实值输出标记的平均值**作为**预测结果**. 

- 还可以基于距离远近进行**加权平均**或**加权投票**,  **距离越近**的样本**权重越大**. 



## 10.1.2 懒惰学习和急切学习

$k$ 近邻学习与前面的学习有一个很大的**不同之处**:  $k$ 近邻学习**没有显式的训练过程**. 

- **懒惰学习**(lazy learning):  此类学习技术在**训练阶段仅仅是把样本保存起来**,  训练时间开销为零,  **待收到测试样本后再进行处理**. 
- **急切学习**(eager learning):  在**训练阶段就对样本进行学习处理**的方法,  称为急切学习.



## 10.1.3 参数 $k$ 重要性

$k$ **是一个重要参数**,  

- 当 **$k$ 取不同值时**,  **分类结果会有显著不同**. 
- 当**采用不同的距离计算方式,**  则找出的"近邻"可能有显著差别,  从而**导致分类结果有显著不同**. 

图 10.1 给出了 $k$ 近邻分类器的一个示意图.

![](https://i.loli.net/2019/07/15/5d2bee05a0ccb50173.png)



## 10.1.4 $k$ 近邻的错误率

给定测试样本 $\boldsymbol x$ , 若其最近邻样本为 $\boldsymbol z$ ,  则最近邻分类器出错的概率就是 $\boldsymbol x$ 与 $\boldsymbol z$ 类别标记不同的概率,  即
$$
\color{red} P(e r r)=1-\sum_{c \in \mathcal{Y}} P(c | \boldsymbol{x}) P(c | \boldsymbol{z})\tag{10.1}
$$
假设样本独立同分布,  且对任意 $\boldsymbol x$ 和任意小正数 $\delta$ ,  在 $\boldsymbol x$ 附近 $\delta$ 距离范围内**总能找到一个训练样本**;  换言之,  对任意测试样本,  总能在任意近的范围内找到式 (10.1) 中的训练样本 $\delta$ . 令 $c^{*}=\arg \max _{c \in \mathcal{Y}} P(c | \boldsymbol{x})$ 表示贝叶斯最优分类器的结果,  有:  
$$
\color{red} \begin{eqnarray*} P(e r r) &=&1-\sum_{c \in \mathcal{Y}} P(c | \boldsymbol{x}) P(c | \boldsymbol{z}) \\ & \simeq& 1-\sum_{c \in \mathcal{Y}} P^{2}(c | \boldsymbol{x}) \\ & \leqslant& 1-P^{2}\left(c^{*} | \boldsymbol{x}\right) \\ &=&\left(1+P\left(c^{*} | \boldsymbol{x}\right)\right)\left(1-P\left(c^{*} | \boldsymbol{x}\right)\right) \\ & \leqslant& 2 \times\left(1-P\left(c^{*} | \boldsymbol{x}\right)\right)\tag{10.2} \end {eqnarray*}
$$

> ------
>
> **注1:  关于式 (10.2) 的推导**
>
> **1 "$\simeq$" 后边:**
>
> 因为是任意小正数 $\delta$ ,  则 $f(x) \simeq f(x+\delta) )$ ,  所以,  $P(c | \boldsymbol{x}) \simeq P(c | \boldsymbol{z})$ 
>
> 2 **第一个 "$\leqslant$" 右边**:
>
>  因为 $P(c^{*}|\boldsymbol x)$ 只是 $\sum_{c \in \mathcal{Y}} P^{2}(c | \boldsymbol{x})$ 的一部分, 因此 $P^{2}\left(c^{*} | \boldsymbol{x}\right) \leqslant \sum_{c \in \mathcal{Y}} P^{2}(c | \boldsymbol{x})$ 

------

可以得到这样一个**结论**:

最近邻分类器虽简单,  但它的**泛化错误率不超过贝叶斯最优分类器**的**错误率**的**两倍**.   





# 10.2 低维嵌入

## 10.2.1 维数灾难的概念

在**高维**情形下出现的**数据样本稀疏**、**距离计算困难**等问题,  是所有机器学习方法共同面临的严重障碍,  被称为**"维数灾难"** (curse of dimensionality). 

**缓解维数灾难**的一个重要途径是**降维 ** (dimension reduction),  亦称**"维数约简"** ,  即通过某种**数学变换**将**原始高维属性空间**转变为一个**低维"子空间"** (subspace),  在这个子空间中**样本密度大幅提高**,  **距离计算也变得更为容易** . 

在很多时候,  人们观测或收集到的数据**样本虽是高维的**,  但**与学习任务密切相关**的也许仅是**某个低维分布**,  即**高维空间中的一个低维"嵌" **(embedding).  

图 10.2 给出一个直观的列子. 

![](https://i.loli.net/2019/07/16/5d2d8ee3a6d4877013.png)

若要求原始空间中样本之间的距离在低维空间中得以保持,  如图 10.2 所示,  即得到**"多维缩放"**( Multiple Dimensional Scaling,  简称 **MDS**) .  MDS 是一种经典的降维方法. 



## 10.2.2 MDS 的数学表示

### 1 MDS 的距离表示

假定 $m$ 个样本在原始空间的距离矩阵为 $\mathbf{D} \in \mathbb{R}^{m \times m}$ ,  其第 $i$ 行 $j$ 列的元素 $dist_{ij}$ 为样本 $\boldsymbol x_{i}$ 到 $\boldsymbol x_{j}$ 的距离. 

目标是获得样本在 $d^{\prime}$ 维空间的表示 $\mathbf{Z} \in \mathbb{R}^{d^{\prime} \times m}, d^{\prime} \leqslant d$ ,  且任意两个样本在 $d^{\prime}$ 维空间中的欧氏距离等于原始空间中的距离,  即 $\left\|z_{i}-z_{j}\right\|=d i s t_{i j}$ .

令 $\mathbf{B}=\mathbf{Z}^{\mathrm{T}} \mathbf{Z} \in \mathbb{R}^{m \times m}$ ,  其中 $\mathbf{B}$ 为降维后样本的内积矩阵,  $b_{i j}=\boldsymbol{z}_{i}^{\mathrm{T}} \boldsymbol{z}_{j}$ ,  有:
$$
\color{red} \begin{aligned} \operatorname{dist}_{i j}^{2} &=\left\|z_{i}\right\|^{2}+\left\|z_{j}\right\|^{2}-2 \boldsymbol{z}_{i}^{\mathrm{T}} \boldsymbol{z}_{j} \\ &=b_{i i}+b_{j j}-2 b_{i j} \end{aligned}\tag{10.3}
$$

------

> **注4: 关于 10.3 的相关推导**
>
> - **关于 $b_{i j}=\boldsymbol{z}_{i}^{\mathrm{T}} \boldsymbol{z}_{j}$** 
>
> 已知 $\mathbf{Z}=\left\{\boldsymbol{z}_{1}, \boldsymbol{z}_{2}, \ldots, \boldsymbol{z}_{i}, \ldots, \boldsymbol{z}_{m}\right\} \in \mathbb{R}^{d^{\prime} \times m}$ ,  其中 $\boldsymbol{z}_{i}=\left(z_{1 i} ; z_{2 i} ; \ldots ; z_{d i^{\prime}}\right) \in \mathbb{R}^{d^{\prime} \times 1}$  ;  降维后的内积矩阵 $\mathbf{B}=\mathbf{Z}^{\mathrm{T}} \mathbf{Z} \in \mathbb{R}^{m \times m}$ ,  其中矩阵 $\mathbf{B}$ 的第 $i$ 行第 $j$ 列元素 $b_{ij}$ ,具体的矩阵表示如下:
>
>  
> $$
> \mathbf{B}=\mathbf{Z}^{\top} \mathbf{Z}=\left[\begin{array}{cccccc}{z_{11}} & {z_{21}} & {\cdots} & {z_{i 1}} & {\cdots} & {z_{m 1}} \\ {z_{12}} & {z_{22}} & {\cdots} & {z_{i 2}} & {\cdots} & {z_{m 2}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {\color{red}z_{1 j}} & {\color{red}z_{2 j}} & {\cdots} & {\color{red}z_{i j}} & {\cdots} & {\color{red}z_{m j}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {z_{1 d^{\prime}}} & {z_{2 d^{\prime}}} & {\cdots} & {z_{i d^{\prime}}} & {\cdots} & {z_{m d^{\prime}}}\end{array}\right] \left[\begin{array}{cccccc}{z_{11}} & {z_{12}} & {\cdots} & {\color{red} z_{1 j}} & {\cdots} & {z_{1 d^{\prime}}} \\ {z_{21}} & {z_{22}} & {\cdots} & {\color{red} z_{2 j}} & {\cdots} & {z_{2 d^{\prime}}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {z_{i 1}} & {z_{i 2}} & {\cdots} & {\color{red} z_{i j}} & {\cdots} & {z_{i d^{\prime}}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {z_{m 1}} & {z_{m 2}} & {\cdots} & {\color{red} z_{m j}} & {\cdots} & {z_{m d^{\prime}}}\end{array}\right]
> $$
> <font color=red>**有几个特别的:**(其中, 红色的就是 $b_{jj}$ )</font>
>
> 1. $\color{red} b_{i i}=\boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{i}=\left\|\boldsymbol{z}_{i}\right\|^{2}$
> 2. $\color{red} b_{j j}=\boldsymbol{z}_{j}^{\top} \boldsymbol{z}_{j}=\left\|\boldsymbol{z}_{j}\right\|^{2}$
> 3. $\color{red} b_{i j}=\boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{j}$
>
> 
>
> - **关于 (10.3) 推导:** 
>
>   因为 $\boldsymbol{z}_{i}-\boldsymbol{z}_{j}$ 是列向量,  则 $\left\|\boldsymbol{z}_{i}-\boldsymbol{z}_{j}\right\|^{2}=\left(\boldsymbol{z}_{i}-\boldsymbol{z}_{j}\right)^{\top}\left(\boldsymbol{z}_{i}-\boldsymbol{z}_{j}\right)$,  则可得:
>   $$
>   \begin{aligned} \operatorname{dist}_{i j}^{2} &=\left\|\boldsymbol{z}_{i}-\boldsymbol{z}_{j}\right\|^{2}=\left(\boldsymbol{z}_{i}-\boldsymbol{z}_{j}\right)^{\top}\left(\boldsymbol{z}_{i}-\boldsymbol{z}_{j}\right) \\ &=\boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{i}-\boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{j}-\boldsymbol{z}_{j}^{\top} \boldsymbol{z}_{i}+\boldsymbol{z}_{j}^{\top} \boldsymbol{z}_{j} \\ &=\boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{i}+\boldsymbol{z}_{j}^{\top} \boldsymbol{z}_{j}-2 \boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{j} \\ &=\left\|\boldsymbol{z}_{i}\right\|^{2}+\left\|\boldsymbol{z}_{j}\right\|^{2}-2 \boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{j} \\ &=b_{i i}+b_{j j}-2 b_{i j} \end{aligned}
>   $$
>   **小注:  因为 $\boldsymbol{z}_{j}^{\top} \boldsymbol{z}_{i}$ 和 $\boldsymbol{z}_{j}^{\top} \boldsymbol{z}_{j}$ 都是一个值, 是一个标量, 因此 $\boldsymbol{z}_{j}^{\top} \boldsymbol{z}_{i}=\boldsymbol{z}_{j}^{\top} \boldsymbol{z}_{j}$ .**
>
>   

------



同时, 为了便于讨论,  令降维后的样本 $\mathbf{Z}$ 被中心化,  即 $\color{red}\sum_{i=1}^{m} \boldsymbol z_{i}=\boldsymbol 0$ . 那么,  矩阵 $\mathbf{B}$ 的行与列之和均为零,  即 $\color{red} \sum_{i=1}^{m} b_{i j}=\sum_{j=1}^{m} b_{i j}=0$ . 

> ------
>
> **注5:  $\sum_{i=1}^{m} b_{i j}=\sum_{j=1}^{m} b_{i j}=0$ 的证明**
> $$
> \begin{array}{l}{\sum_{i=1}^{m} b_{i j}=\sum_{i=1}^{m} \boldsymbol{z}_{j}^{\top} \boldsymbol{z}_{i}=\boldsymbol{z}_{j}^{\top} \sum_{i=1}^{m} \boldsymbol{z}_{i}=\boldsymbol{z}_{j}^{\top} \cdot \mathbf{0}_{d^{\prime} \times 1}=0} \\ {\sum_{j=1}^{m} b_{i j}=\sum_{j=1}^{m} \boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{j}=\boldsymbol{z}_{i}^{\top} \sum_{j=1}^{m} \boldsymbol{z}_{j}=\boldsymbol{z}_{i}^{\top} \cdot \mathbf{0}_{d^{\prime} \times 1}=0}\end{array}
> $$

------

易得:

$$
\color{red}\sum_{i=1}^{m} d i s t_{i j}^{2}=\operatorname{tr}(\mathbf{B})+m b_{j j}\tag{10.4}
$$

$$
\color{red}\sum_{j=1}^{m} d i s t_{i j}^{2}=\operatorname{tr}(\mathbf{B})+m b_{i i}\tag{10.5}
$$

$$
\color{red}\sum_{i=1}^{m} \sum_{j=1}^{m} d i s t_{i j}^{2}=2 m \operatorname{tr}(\mathbf{B})\tag{10.6}
$$

> ------
>
> **注6: 关于 (10.4), (10.5) 和 (10.6) 的证明** (根据 (10.3))
>
> (两个证明, 一个是根据 (10.3) 第一个等式, 一个根据第二个等式)
>
> <font color=red>根据第一个等式 $\operatorname{dist}_{i j}^{2} =\left\|z_{i}\right\|^{2}+\left\|z_{j}\right\|^{2}-2 \boldsymbol{z}_{i}^{\mathrm{T}} \boldsymbol{z}_{j}$ </font> 来进行证明.
>
> ------
>
> 式 (10.4) 证明:
> $$
> \begin{aligned} \sum_{i=1}^{m} d i s t_{i j}^{2} &=\sum_{i=1}^{m}\left(\left\|\boldsymbol{z}_{i}\right\|^{2}+\left\|\boldsymbol{z}_{j}\right\|^{2}-2 \boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{j}\right) \\ &=\sum_{i=1}^{m}\left\|\boldsymbol{z}_{i}\right\|^{2}+\sum_{i=1}^{m}\left\|\boldsymbol{z}_{j}\right\|^{2}-2 \sum_{i=1}^{m} \boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{j} \end{aligned}
> $$
> 又根据定义可知:
> $$
> \color{red} \begin{array}{l}{\sum_{i=1}^{m}\left\|\boldsymbol{z}_{i}\right\|^{2}=\sum_{i=1}^{m} \boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{i}=\sum_{i=1}^{m} b_{i i}=\operatorname{tr}(\mathbf{B})} \\ {\sum_{i=1}^{m}\left\|\boldsymbol{z}_{j}\right\|^{2}=\left\|\boldsymbol{z}_{j}\right\|^{2} \sum_{i=1}^{m} 1=m\left\|\boldsymbol{z}_{j}\right\|^{2}=m \boldsymbol{z}_{j}^{\top} \boldsymbol{z}_{j}=m b_{j j}}\end{array}
> $$
> 且:
> $$
> \sum_{i=1}^{m} z_{i}^{\top} \boldsymbol{z}_{j}=\left(\sum_{i=1}^{m} \boldsymbol{z}_{i}^{\top}\right) \boldsymbol{z}_{j}=\mathbf{0}_{1 \times d^{\prime}} \cdot \boldsymbol{z}_{j}=0
> $$
> 带入可得:
> $$
> \begin{aligned} \sum_{i=1}^{m} d i s t_{i j}^{2} &=\sum_{i=1}^{m}\left\|\boldsymbol{z}_{i}\right\|^{2}+\sum_{i=1}^{m}\left\|\boldsymbol{z}_{j}\right\|^{2}-2 \sum_{i=1}^{m} \boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{j} \\ &=\operatorname{tr}(\mathbf{B})+m b_{j j} \end{aligned}
> $$
> 
>
> **(10.5)类似可得**
>
> 
>
> (10.6) 推导:
> $$
> \begin{aligned} \sum_{i=1}^{m} \sum_{j=1}^{m} d i s t_{i j}^{2} &=\sum_{i=1}^{m} \sum_{j=1}^{m}\left(\left\|\boldsymbol{z}_{i}\right\|^{2}+\left\|\boldsymbol{z}_{j}\right\|^{2}-2 \boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{j}\right) \\ &=\sum_{i=1}^{m} \sum_{j=1}^{m}\left\|\boldsymbol{z}_{i}\right\|^{2}+\sum_{i=1}^{m} \sum_{j=1}^{m}\left\|\boldsymbol{z}_{j}\right\|^{2}-2 \sum_{i=1}^{m} \sum_{j=1}^{m} \boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{j} \\ &=2 m \operatorname{tr}(\mathbf{B}) \end{aligned}
> $$
> 其中各项的求解如下:
> $$
> \begin{array}{l}{\sum_{i=1}^{m} \sum_{j=1}^{m}\left\|\boldsymbol{z}_{i}\right\|^{2}=\sum_{i=1}^{m}\left(\left\|\boldsymbol{z}_{i}\right\|^{2} \sum_{j=1}^{m} 1\right)=m \sum_{i=1}^{m}\left\|\boldsymbol{z}_{i}\right\|^{2}=m \operatorname{tr}(\mathbf{B})} \\ {\sum_{i=1}^{m} \sum_{j=1}^{m}\left\|\boldsymbol{z}_{j}\right\|^{2}=\sum_{i=1}^{m} \operatorname{tr}(\mathbf{B})=m \operatorname{tr}(\mathbf{B})} \\ {\sum_{i=1}^{m} \sum_{j=1}^{m} \boldsymbol{z}_{i}^{\top} \boldsymbol{z}_{j}=0}\end{array}
> $$
> 即可得证.
>
> ------
>
> <font color=red>根据 (10.3)第二个等式证明, 即 $\operatorname{dist}_{i j}^{2}==b_{i i}+b_{j j}-2 b_{i j}$ </font>
>
> ------
>
> 式 (10.4) 证明:
> $$
> \begin{aligned} \sum_{i=1}^{m} d i s t_{i j}^{2} &=\sum_{i=1}^{m}\left(b_{i i}+b_{j j}-2 b_{i j}\right) \\ &=\sum_{i=1}^{m} b_{i i}+\sum_{i=1}^{m} b_{j j}-2 \sum_{i=1}^{m} b_{i j} \\ &=\operatorname{tr}(\boldsymbol B)+mb_{j j}-0 \\ &=\operatorname{tr}(\boldsymbol B)+m b_{jj}  \end{aligned}
> $$
> 同理可证式 (10.5)
>
> 式 (10.6) 证明:
> $$
> \begin{aligned} \sum_{i=1}^{m} \sum_{j=1}^{m} d i s t_{i j}^{2} &=\sum_{i=1}^{m} \sum_{j=1}^{m}(b_{i i}+b_{j j}-2 b_{i j}) \\ &=\sum_{i=1}^{m} \sum_{j=1}^{m}b_{i i}+\sum_{i=1}^{m} \sum_{j=1}^{m}b_{j j}-2 \sum_{i=1}^{m} \sum_{j=1}^{m} b_{i j} \\ &=\sum_{i=1}^{m} b_{i i} \cdot \sum_{j=1}^{m} \cdot 1 +\sum_{j=1}^{m} b_{j j} \cdot \sum_{i=1}^{m} \cdot 1+\sum_{i=1}^{m} \sum_{j=1}^{m} b_{i j} \\
> &=m \operatorname{tr}(\boldsymbol B)+m \operatorname{tr}(\boldsymbol B)-2 \sum_{i=1}^{m} \sum_{j=1}^{m} b_{i j} \\
> &=2m\operatorname{tr}(\boldsymbol B)
> \end{aligned}
> $$
> 注意 $\sum_{i=1}^{m} \sum_{j=1}^{m} b_{i j}=0$
> $$
> \begin{aligned}
> \sum_{i=1}^{m} \sum_{j=1}^{m} b_{i j} = \sum_{j=1}^{m} \cdot \sum_{j=1}^{m} b_{i j} =0
> 
> \end{aligned}
> $$

------



其中,  $\operatorname{tr}(\cdot)$ 表示矩阵的迹 (trace),  $\operatorname{tr}(\mathbf{B})=\sum_{i=1}^{m}\left\|\boldsymbol{z}_{i}\right\|^{2}$ ,  令
$$
\begin{eqnarray*} \operatorname{dist}_{i \cdot}^{2} &=&\frac{1}{m} \sum_{j=1}^{m} d i s t_{i j}^{2}\tag{10.7} \\ d i s t_{\cdot j}^{2} &=&\frac{1}{m} \sum_{i=1}^{m} d i s t_{i j}^{2}\tag{10.8} \\ d i s t_{ \cdot \cdot}^{2} &=&\frac{1}{m^{2}} \sum_{i=1}^{m} \sum_{j=1}^{m} d i s t_{i j}^{2}\tag{10.9} \end{eqnarray*}
$$


根据式 (10.3) 和式 (10.4)~(10.9),  易得:
$$
\color{red} b_{i j}=-\frac{1}{2}\left(d i s t_{i j}^{2}-d i s t_{i \cdot}^{2}-d i s t_{\cdot j}^{2}+d i s t_{\cdot \cdot}^{2}\right)\tag{10.10}
$$
**由此即可通过降维前后不变的距离矩阵 $\mathbf{D}$ 求取内积矩阵 $\mathbf{B}$ .**



### 2 MDS 的特征值求解及完整算法描述

对矩阵 $\mathbf{B}$ 做特征值分解 (eigenvalue decomposition),  $\mathbf{B}=\mathbf{V} \mathbf{\Lambda} \mathbf{V}^{\mathrm{T}}$ ( 注:  $\mathbf{B}$ 是对称矩阵),  其中, $\mathbf{\Lambda}=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \dots, \lambda_{d}\right)$ 为特征值构成的对角矩阵,  $\lambda_{1} \geqslant \lambda_{2} \geqslant \ldots \geqslant \lambda_{d}$ ,  $\mathbf{V}$ 为特征向量矩阵. 假定其中有 $d^{*}$ 个非零特征值,  它们构成对角矩阵 $\mathbf{\Lambda}_{*}=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{d^{*}}\right)$ ,  令 $\mathbf{V^{*}}$ 表示相应的特征向量矩阵,  则 $\mathbf{Z}$ 可表达为:
$$
\mathbf{Z}=\mathbf{\Lambda}_{*}^{1 / 2} \mathbf{V}_{*}^{\mathrm{T}} \in \mathbb{R}^{d^{*} \times m}\tag{10.11}
$$

> 注7: 式 (10.11) 目前不知道如何来的, <font color=red>暂放, 标记.</font>



而在现实中, 为了有效降维,  往往仅需降维后的距离与原始空间中的距离尽可能接近,  而不必严格相等.  此时可取 $d^{\prime} \ll d$ 个最大特征值构成对角矩阵 $\tilde{\mathbf{\Lambda}}=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{d^{\prime}}\right)$ ,  令 $\tilde{\mathbf{\Lambda}}$ 表示相应的特征向量矩阵, 则 $\mathbf{Z}$ 可表达为:
$$
\mathbf{Z}=\tilde{\mathbf{\Lambda}}^{1 / 2} \tilde{\mathbf{V}}^{\mathrm{T}} \in \mathbb{R}^{d^{\prime} \times m}\tag{10.12}
$$
图 10.3 给出了 MDS 算法的描述.

![](https://i.loli.net/2019/07/18/5d2fbd9fa4be592170.png)



## 10.2.3 高维空间的线性变换

 一般来说,  欲获得低维子空间,  最简单的是对**原始高维空间进行线性变换**. 给定 $d$  维空间中的样本 $\mathbf{X}=\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right) \in \mathbb{R}^{d \times m}$ ,  变换之后得到 $d^{\prime} \leqslant d$ 维空间中的样本:
$$
\mathbf{Z}=\mathbf{W}^{\mathrm{T}} \mathbf{X}\tag{10.13}
$$
其中, $\mathbf{W} \in \mathbb{R}^{d \times d^{\prime}}$ 是变换矩阵,  $\mathbf{Z} \in \mathbb{R}^{d^{\prime} \times m}$ 是样本在新空间中的表示.







# 10.3 主成分分析

## 10.3.1 主成分分析的基本概念

**主成分分析 (Principal Component Analysis,  简称 PCA)** 是最常用的一种**降维方法**.

对于**正交属性空间中的样本点**,  如何用一个**超平面**对所有样本进行恰当的表达?

若存在这样的超平面,  它大概具有这样的性质:

- **最近重构性**:  样本点到这个超平面的距离都**足够近**;
- **最大可分性**:  样本点在这个超平面上的**投影尽可能分开**.

基于最近重构性和最大可分性,  能分别得到主成分分析的**两种等价推导**.



## 10.3.2 基于最近重构性推导主成分分析

### 1 基本假定和条件

首先,  假定数据样本进行了中心化,  也即 $\sum_{i} \boldsymbol{x}_{i}=\mathbf{0}$ ;

同时, 再假定投影变换后得到的新坐标系为 $\left\{\boldsymbol{w}_{1}, \boldsymbol{w}_{2}, \ldots, \boldsymbol{w}_{d}\right\}$ ,  其中,  $\boldsymbol{w}_{i}$ 是标准正交基向量,  也即是满足 $\left\|\boldsymbol{w}_{i}\right\|_{2}=1, \boldsymbol{w}_{i}^{\mathrm{T}} \boldsymbol{w}_{j}=0(i \neq j)$ 

若丢弃新坐标系中的部分坐标,  即将纬度降低到 $d^{\prime}<d$ ,  则样本点 $\boldsymbol x_{i}$ 在低维坐标系中的投影是 $\boldsymbol z_{i}=\left(z_{i 1} ; z_{i 2} ; \ldots ; z_{i d^{\prime}}\right)$ ,  其中 $z_{i j}=\boldsymbol{w}_{j}^{\mathrm{T}} \boldsymbol{x}_{i}$ 是 $\boldsymbol x_{i}$ 在低维坐标系下第 $j$ 维坐标.  若基于 $\boldsymbol z_{i}$ 来重构 $\boldsymbol x_{i}$ ,  则会得到 $\hat{\boldsymbol{x}}_{i}=\sum_{j=1}^{d^{\prime}} z_{i j} \boldsymbol{w}_{j}$ .

> ------
>
> **注8:  关于1, 2, 3 的解释和推导**
>
> **对于 $1$ :** **理解中心化的概念**
>
> 因为数据进行了中心化, 易知:  $\sum_{i} \boldsymbol{x}_{i}=\mathbf{0}$ 
>
> **对于 $2$ :** **理解标准正交基的概念**
>
> - 先理解 **基** 的概念, 根据<线性代数>(第五版) $p141$ 关于基的定义:
>
> ![](https://raw.githubusercontent.com/xhtxzwj/picfiles/master/20190718214312.png)
>
> - 再理解**正交**的概念: <线性代数>(第五版) $p112$ 关于正交的概念:
>
> 当两个向量的**内积**为 $0$ 时(即 $[\boldsymbol x, \boldsymbol y]=0$ ) ,  称向量 $\boldsymbol x$ 与 $\boldsymbol y$ 正交. 同时注意到 $[\boldsymbol x, \boldsymbol y]= \boldsymbol x^{T} \boldsymbol y$ , 其中  $\boldsymbol x$ 与 $\boldsymbol y$ 都是列向量.
>
> - 最后**规范正交基**的概念:  <线性代数>(第五版) $p113$ 关于规范正交基(也叫标准正交基)的定义:
>
> ![](https://raw.githubusercontent.com/xhtxzwj/picfiles/master/20190718215417.png)
>
> 
>
> <font color=red>对于3:  理解坐标变换和基于 $\boldsymbol z_{i}$ 来重构 $\boldsymbol x_{i}$ </font>
>
> - <font color=red>先理解坐标的表示</font>,  根据<线性代数>(第五版) $P113$ 关于标准正交基中的坐标计算公式
>
>   若 $e_{1}, \cdots, e_{r}$ 是 $V$ 的一个规范正交基,  那么 $V$ 中任一向量 $\boldsymbol a$ 应能由 $e_{1}, \cdots, e_{r}$ 线性表示,  设表示式为
>   $$
>   \boldsymbol a=\lambda_{1} \boldsymbol e_{1}+\lambda_{2} \boldsymbol e_{2}+\cdots+\lambda_{r} \boldsymbol e_{r}
>   $$
>   为求其中的系数 $\lambda_{i}(i=1, \cdots, r)$ ,  可用 $\boldsymbol e_{i}^{T}$ 左乘上式, 有
>   $$
>   \boldsymbol e_{i}^{\mathrm{T}} \boldsymbol a=\lambda_{i} \boldsymbol e_{i}^{\mathrm{T}} \boldsymbol e_{i}=\lambda_{i}
>   $$
>   即 
>   $$
>   \color{red} \lambda_{i}=\boldsymbol e_{i}^{\mathrm{T}} \boldsymbol a=\left[\boldsymbol a, \boldsymbol  e_{i}\right]
>   $$
>   **这就是向量在规范正交基中的坐标的计算公式. 利用这个公式能方便地求得向量的坐标.** 
>
>   再看书中的投影是 $\boldsymbol z_{i}=\left(z_{i 1} ; z_{i 2} ; \ldots ; z_{i d^{\prime}}\right)$ , $\boldsymbol z_{i}$ 也就是坐标向量,   $(z_{i 1} ; z_{i 2} ; \ldots ; z_{i d^{\prime}}$) 也就是各个坐标系数,  那么  $\boldsymbol z_{i}= (z_{i 1} ; z_{i 2} ; \ldots ; z_{i d^{\prime}})= \left(\boldsymbol{w}_{1}^{\top} \boldsymbol{x}_{i} ; \boldsymbol{w}_{2}^{\top} \boldsymbol{x}_{i} ; \ldots ; \boldsymbol{w}_{d^{\prime}}^{\top} \boldsymbol{x}_{i}\right)$ . 易知,  $\color{red}z_{i j}=\boldsymbol{w}_{j}^{\mathrm{T}} \boldsymbol{x}_{i}$ 
>
> - <font color=red>再来看坐标表示的过程 </font>
>
>   对于 $d$ 维空间 $ \mathbb{R}^{d \times 1}$ 来说,  传统的坐标系为 $\left\{\boldsymbol{v}_{1}, \boldsymbol{v}_{2}, \ldots, \boldsymbol{v}_{k}, \dots, \boldsymbol{v}_{d}\right\}$ (标准正交基),  其中 $\boldsymbol v_{k}$ 为除第 $k$ 个元素为 $1$ 其余元素均 $0$ 的 $d$ 维列向量;  此时对于样本点 $\boldsymbol{x}_{i}=\left(x_{i 1} ; x_{i 2} ; \ldots ; x_{i d}\right) \in \mathbb{R}^{d \times 1}$ 来说亦可表示为 : $\boldsymbol{x}_{i}=x_{i 1} \boldsymbol{v}_{1}+x_{i 2} \boldsymbol{v}_{2}+\ldots+x_{i d} \boldsymbol{v}_{d}$ ,
>
>   
>
>   现在假定投影变换后得到的新坐标系为 $\left\{\boldsymbol{w}_{1}, \boldsymbol{w}_{2}, \ldots, \boldsymbol{w}_{k}, \dots, \boldsymbol{w}_{d}\right\}$ (即一组新的标准正交基),  那么根据前面的坐标表示, 我们可以得到 $\boldsymbol x_{i}$ 在新坐标系中的坐标为 $\left(\boldsymbol{w}_{1}^{\top} \boldsymbol{x}_{i} ; \boldsymbol{w}_{2}^{\top} \boldsymbol{x}_{i} ; \ldots ; \boldsymbol{w}_{d}^{\top} \boldsymbol{x}_{i}\right)$ .
>
>   
>
>   若丢弃新坐标系中的部分坐标,  即将维度降低到 $d^{\prime}<d$ ,  并令
>   $$
>   \mathbf{W}=\left(\boldsymbol{w}_{1}, \boldsymbol{w}_{2}, \ldots, \boldsymbol{w}_{d^{\prime}}\right) \in \mathbb{R}^{d \times d^{\prime}}
>   $$
>   <font color=red>则 $\boldsymbol x_{i}$ 在低维坐标系中的投影为:</font> 
>   $$
>   \color{red} \begin{aligned} \boldsymbol{z}_{i} &=\left(z_{i 1} ; z_{i 2} ; \ldots ; z_{i d^{\prime}}\right)=\left(\boldsymbol{w}_{1}^{\top} \boldsymbol{x}_{i} ; \boldsymbol{w}_{2}^{\top} \boldsymbol{x}_{i} ; \ldots ; \boldsymbol{w}_{d^{\prime}}^{\top} \boldsymbol{x}_{i}\right) \\ &=\mathbf{W}^{\top} \boldsymbol{x}_{i} \in \mathbb{R}^{d^{\prime} \times 1} \end{aligned}
>   $$
>   同时也易知,  $\color{red}z_{i j}=\boldsymbol{w}_{j}^{\mathrm{T}} \boldsymbol{x}_{i}$ .
>
> - <font color=red>最后来看基于 $\boldsymbol z_{i}$ 来重构 $\boldsymbol x_{i}$ , 得到 $\hat{\boldsymbol{x}}_{i}=\sum_{j=1}^{d^{\prime}} z_{i j} \boldsymbol{w}_{j}$ </font>
>
>   此处,  卡了我非常久的时间,  我之前老是从式子本身来推导, 发现如何都推不出来. 后来发现, 自己想多了,可能也是自己脑子不太灵光.   现在, 特别记录在此, 防止以后有人看到这里, 会有一样疑惑. 
>
>   <font color=red> 其实,  从坐标表示的定义就可以直接得到.</font>
>
>   看前面的坐标表示过程
>   $$
>   \boldsymbol{x}_{i}=x_{i 1} \boldsymbol{v}_{1}+x_{i 2} \boldsymbol{v}_{2}+\ldots+x_{i d} \boldsymbol{v}_{d}
>   $$
>   也即是
>   $$
>   \boldsymbol x_{i}=\sum_{j=1}^{d} \boldsymbol x_{ij} \boldsymbol v_{j}
>   $$
>   当坐标系从 $\left\{\boldsymbol{v}_{1}, \boldsymbol{v}_{2}, \ldots, \boldsymbol{v}_{k}, \dots, \boldsymbol{v}_{d}\right\}$ 换成  $\left\{\boldsymbol{w}_{1}, \boldsymbol{w}_{2}, \ldots, \boldsymbol{w}_{d}\right\}$ , 丢弃部分坐标, 即 $d^{\prime}<d$ , 那么就是
>
>   
>   $$
>   \hat {\boldsymbol x}_{i}=\sum_{j=1}^{d^{\prime}} \boldsymbol z_{ij} \boldsymbol w_{j}=\mathbf{W} z_{i}
>   $$

------



### 2 样本点 $\boldsymbol x_{i}$ 与投影重构的样本点 $\hat {\boldsymbol x_{i}}$ 之间的距离

考虑整个训练集,  样本点 $\boldsymbol x_{i}$ 与投影重构的样本点 $\hat {\boldsymbol x_{i}}$ 之间的距离为:
$$
\begin{aligned}
\sum_{i=1}^{m}\left\|\sum_{j=1}^{d^{\prime}} z_{i j} \boldsymbol w_{j}-\boldsymbol{x}_{i}\right\|^{2}&=\sum_{i=1}^{m}\left\|\mathbf{W} \boldsymbol{z}_{i}-\boldsymbol{x}_{i}\right\|_{2}^{2} \\
&=\sum_{i=1}^{m}\left\|\mathbf{W} \mathbf{W}^{\top} \boldsymbol{x}_{i}-\boldsymbol{x}_{i}\right\|_{2}^{2} \\
&=\sum_{i=1}^{m}\left(\mathbf{W} \mathbf{W}^{\top} \boldsymbol{x}_{i}-\boldsymbol{x}_{i}\right)^{\top}\left(\mathbf{W} \mathbf{W}^{\top} \boldsymbol{x}_{i}-\boldsymbol{x}_{i}\right) \\
&=\sum_{i=1}^{m}\left(\boldsymbol{x}_{i}^{\top} \mathbf{W} \mathbf{W}^{\top} \mathbf{W} \mathbf{W}^{\top} \boldsymbol{x}_{i}-2 \boldsymbol{x}_{i}^{\top} \mathbf{W} \mathbf{W}^{\top} \boldsymbol{x}_{i}+\boldsymbol{x}_{i}^{\top} \boldsymbol{x}_{i}\right) \\
&=\sum_{i=1}^{m}\left(\boldsymbol{x}_{i}^{\top} \mathbf{W} \mathbf{W}^{\top} \boldsymbol{x}_{i}-2 \boldsymbol{x}_{i}^{\top} \mathbf{W} \mathbf{W}^{\top} \boldsymbol{x}_{i}+\boldsymbol{x}_{i}^{\top} \boldsymbol{x}_{i}\right) \\
&=\sum_{i=1}^{m}\left(-\boldsymbol{x}_{i}^{\top} \mathbf{W} \mathbf{W}^{\top} \boldsymbol{x}_{i}+\boldsymbol{x}_{i}^{\top} \boldsymbol{x}_{i}\right) \\
&=\sum_{i=1}^{m}\left(-\left(\mathbf{W}^{\top} \boldsymbol{x}_{i}\right)^{\top}\left(\mathbf{W}^{\top} \boldsymbol{x}_{i}\right)+\boldsymbol{x}_{i}^{\top} \boldsymbol{x}_{i}\right) \\
&=\sum_{i=1}^{m}\left(-\left\|\mathbf{W}^{\top} \boldsymbol{x}_{i}\right\|_{2}^{2}+\boldsymbol{x}_{i}^{\top} \boldsymbol{x}_{i}\right) \\
&\propto -\sum_{i=1}^{m}\left\|\mathbf{W}^{\top} \boldsymbol{x}_{i}\right\|_{2}^{2}
\end{aligned}
$$

> ------
>
> 注9: 上式的推导过程
>
> **第四个等式到第五个等式**:
>
> 由于 $\boldsymbol{w}_{i}^{\top} \boldsymbol{w}_{j}=0,(i \neq j),\left\|\boldsymbol{w}_{i}\right\|=1$ , 且 $\mathbf{W}=\left(\boldsymbol{w}_{1}, \boldsymbol{w}_{2}, \ldots, \boldsymbol{w}_{d^{\prime}}\right) \in \mathbb{R}^{d \times d^{\prime}} $ ,  那么 $\mathbf{W}^{\top} \mathbf{W}=\mathbf{I} \in \mathbb{R}^{d^{\prime} \times d^{\prime}}$ . 即得.
>
> 
>
> **第八个等式到最后一个式子:** 
>
> 由于是寻找 $\mathbf{W}$ 使得目标函数最小,  而 $\boldsymbol{x}_{i}^{\top} \boldsymbol{x}_{i}$ 与 $\mathbf{W}$ 无关,  因此, 优化时可以略去.

------



同时,  令 $\mathbf{X}=\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right) \in \mathbb{R}^{d \times m}$ ,  再根据前面的预备知识中关于矩阵范数中的知识,  即 $\|\mathbf{A}\|_{F}^{2}=\sum_{j=1}^{n}\left\|\boldsymbol{\alpha}_{j}\right\|_{2}^{2}$ .  那么上面的式子可以继续化简为:
$$
\begin{eqnarray*}-\sum_{i=1}^{m}\left\|\mathbf{W}^{\top} \boldsymbol{x}_{i}\right\|_{2}^{2} &=&-\left\|\mathbf{W}^{\top} \mathbf{X}\right\|_{F}^{2} \\ &=&-\operatorname{tr}\left(\left(\mathbf{W}^{\top} \mathbf{X}\right)\left(\mathbf{W}^{\top} \mathbf{X}\right)^{\top}\right) \\ &=&-\operatorname{tr}\left(\mathbf{W}^{\top} \mathbf{X} \mathbf{X}^{\top} \mathbf{W}\right) \tag{10.14}\end{eqnarray*}
$$

> ------
>
> **注10:  (10.14) 的推导**
>
> **第一个等式:**
>
> 根据 $\|\mathbf{A}\|_{F}^{2}=\sum_{j=1}^{n}\left\|\boldsymbol{\alpha}_{j}\right\|_{2}^{2}$ 即可推得:
>
> $\sum_{i=1}^{m}\|\mathbf{W}^{\top} \boldsymbol{x}_{i}\|_{2}^{2} =\|\mathbf{W}^{\top} \mathbf{X}\|_{F}^{2}$
>
> **第二个等式:**
>
> $\|\mathbf{A}\|_{F}^{2} =\operatorname{tr}(\mathbf{A}^{\top} \mathbf{A})$
>
> 即可得

------

那么根据最近重构性,  就可以得到最终的优化目标和约束条件,  即:
$$
\begin{array}{l}{\min _{\mathbf{W}}-\operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{X} \mathbf{X}^{\mathrm{T}} \mathbf{W}\right)} \\ {\text { s.t. } \mathbf{W}^{\mathrm{T}} \mathbf{W}=\mathbf{I}}\end{array}\tag{10.15}
$$
这就是主成分分析的优化目标. 

同时注意一点, <font color=red> $\sum_{i} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\mathrm{T}}=\mathbf{X} \mathbf{X}^{\top}$ 是协方差矩阵. </font> 

> 注​11:  **本笔记中的式 (10.14) 和书本上的式 (10.14) 有些许差别**
>
> 先给出结论:  $\sum_{i=1}^{m} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}=\mathbf{X} \mathbf{X}^{\top}$ 
>
> 具体证明过程略,  主要是各自展开左右两式子即可得证.





## 10.3.3 基于最大可分性推导主成分分析

从最大可分性出发,  能得到主成分分析的另一种解释. 

样本点 $\boldsymbol x_{i}$ 在新空间中超平面上的投影是 $\mathbf{W}^{\mathrm{T}} \boldsymbol{x}_{i}$ .  若所有样本点的投影能尽可能分开,  则应该使投影后样本点的方差最大化,  如图 10.4 所示:

![](https://i.loli.net/2019/07/19/5d3186063bf0b73562.png)

投影后样本点的方差是 $\sum_{i} \mathbf{W}^{\mathrm{T}} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\mathrm{T}} \mathbf{W}$ ,  于是优化目标可以写成: 
$$
\begin{array}{l}{\max _{\mathbf{W}} \operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{X} \mathbf{X}^{\mathrm{T}} \mathbf{W}\right)} \\ {\text { s.t. } \mathbf{W}^{\mathrm{T}} \mathbf{W}=\mathbf{I}}\end{array}\tag{10.16}
$$

> ------
>
> **注12:  关于式 (10.16) 的推导**
>
> 先考虑协方差矩阵 $\mathbf{X} \mathbf{X}^{\top}$ :
> $$
> \frac{1}{m} \mathbf{X X}^{\top}=\frac{1}{m}\left[\begin{array}{cccc}{\sum_{i=1}^{m} x_{i 1} x_{i 1}} & {\sum_{i=1}^{m} x_{i 1} x_{i 2}} & {\cdots} & {\sum_{i=1}^{m} x_{i 1} x_{i d}} \\ {\sum_{i=1}^{m} x_{i 2} x_{i 1}} & {\sum_{i=1}^{m} x_{i 2} x_{i 2}} & {\cdots} & {\sum_{i=1}^{m} x_{i 2} x_{i d}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {\sum_{i=1}^{m} x_{i d} x_{i 1}} & {\sum_{i=1}^{m} x_{i d} x_{i 2}} & {\cdots} & {\sum_{i=1}^{m} x_{i d} x_{i d}}\end{array}\right]_{d \times d}
> $$
> 
>
> 那么我们可以知道 $\frac{1}{m} \mathbf{X} \mathbf{X}^{\top}$ 的第 $i$ 行第 $j$ 列的元素表示 $\mathbf{X}$ 中第 $i$ 行和 $\mathbf{X}^{\top}$ 中第 $j$ 列( 其实也就是 $\mathbf{X}$ 中第 $j$ 行)的方差(当 $i=j$ )或协方差(当 $i \neq j$ ) .  同时, 我们可以看到,  <font color=red>协方差矩阵的对角线元素为隔行的方差.</font>
>
> 
>
> 对于 $\mathbf{X}=\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right) \in \mathbb{R}^{d \times m}$ ,  将其投影为 $\mathbf{Z}=\left(\boldsymbol{z}_{1}, \boldsymbol{z}_{2}, \ldots, \boldsymbol{z}_{m}\right) \in \mathbb{R}^{d^{\prime} \times m}$ ,  其中 $\mathbf{Z}=\mathbf{W}^{\top} \mathbf{X}$ ,  其中 $\mathbf{W}=\left\{\boldsymbol{w}_{1}, \boldsymbol{w}_{2}, \ldots, \boldsymbol{w}_{d^{\prime}}\right\} \in \mathbb{R}^{d \times d^{\prime}}$ 为一组新的标准正交基.  
>
> 从最大可分性出发,  我们希望在新空间的每一维坐标轴上样本都尽可能分散 (即每维特征尽可能分散,  也就是**各行方差最大**) 
>
> 即寻找 $\mathbf{W} \in \mathbb{R}^{d \times d}$ 使协方差矩阵 $\frac{1}{m} \mathbf{Z} \mathbf{Z}^{\top}$ 对角线元素之和 (矩阵的迹) 最大 (即使各行方差之和最大) . 同时, $\mathbf{Z}=\mathbf{W}^{\top} \mathbf{X}$ , 且  $\frac{1}{m}$ 为常数, 不影响优化过程. **求矩阵对角线元素之和即为矩阵的迹.**  即可得:
> $$
> {\max _{\mathbf{W}} \operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{X} \mathbf{X}^{\mathrm{T}} \mathbf{W}\right)}
> $$

------



显然,  式 (10.16) 与式 (10.15) 是等价的.

对式 (10.15) 或式 (10.16) 使用拉格朗日乘子法可得:
$$
\mathbf{X} \mathbf{X}^{\top} \mathbf{W}=\mathbf{W} \mathbf{\Lambda}\tag{10.17}
$$
其中,  $\Lambda=\left[\begin{array}{cccc}{\lambda_{1}} & {} & {} & {} \\ {} & {\lambda_{2}} & {} & {} \\ {} & {} & {\ddots} & {} \\ {} & {} & {} & {\lambda_{d^{\prime}}}\end{array}\right] \in \mathbb{R}^{d^{\prime} \times d^{\prime}}$ , 

还可以进一步将此式拆成 $d^{\prime}$ 个子式子:
$$
\mathbf{X X}^{\top} \boldsymbol{w}_{i}=\lambda_{i} \boldsymbol{w}_{i}, 1 \leqslant i \leqslant d^{\prime}
$$


> ------
>
> **注13: 关于式 (10.17) 的推导**
>
> 注意若要对式 (10.16) 使用拉格朗日乘子法应先将最大化问题转为式 (10.15) 最小化问题.  对式 (10.15) 使用拉格朗日乘子法,  写出拉格朗日函数:
> $$
> L(\mathbf{W}, \boldsymbol{\Lambda})=-\operatorname{tr}\left(\mathbf{W}^{\top} \mathbf{X} \mathbf{X}^{\top} \mathbf{W}\right)+\left(\mathbf{W}^{\top} \mathbf{W}-\mathbf{I}\right) \Lambda
> $$
> 其中,   $\Lambda=\left[\begin{array}{cccc}{\lambda_{1}} & {} & {} & {} \\ {} & {\lambda_{2}} & {} & {} \\ {} & {} & {\ddots} & {} \\ {} & {} & {} & {\lambda_{d^{\prime}}}\end{array}\right] \in \mathbb{R}^{d^{\prime} \times d^{\prime}}$ ,  $\mathbf{I}=\left[\begin{array}{llll}{1} & {} \\ {} & {1} \\ {} & {} & {\ddots} \\ {} & {} & {} & {1}\end{array}\right] \in \mathbb{R}^{d^{\prime} \times d^{\prime}}$ .
>
> 对 $\mathbf{W} \in \mathbb{R}^{d \times d}$ 求导:
> $$
> \begin{aligned} \frac{\partial L(\mathbf{W}, \boldsymbol{\Lambda})}{\partial \mathbf{W}} &=-\frac{\partial \operatorname{tr}\left(\mathbf{W}^{\top} \mathbf{X} \mathbf{X}^{\top} \mathbf{W}\right)}{\partial \mathbf{W}}+\frac{\partial\left(\mathbf{W}^{\top} \mathbf{W}-\mathbf{I}\right)}{\partial \mathbf{W}} \Lambda \\ &=-\mathbf{X} \mathbf{X}^{\top} \mathbf{W}-\left(\mathbf{X} \mathbf{X}^{\top}\right)^{\top} \mathbf{W}+2 \mathbf{W} \Lambda \\ &=-2 \mathbf{X} \mathbf{X}^{\top} \mathbf{W}+2 \mathbf{W} \mathbf{\Lambda} \end{aligned}
> $$
> 关于向量的求导前面已经说过了,  关于迹的求导, 目前还不清楚,  结果参考开头博客.
>
> 令偏导 $\frac{\partial L(\mathbf{W}, \mathbf{\Lambda})}{\partial \mathbf{W}}=0$ , 可得:
> $$
> \mathbf{X} \mathbf{X}^{\top} \mathbf{W}=\mathbf{W} \mathbf{\Lambda} 
> $$
> 还可以进一步将此式拆成 $d^{\prime}$ 个子式子:
> $$
> \mathbf{X X}^{\top} \boldsymbol{w}_{i}=\lambda_{i} \boldsymbol{w}_{i}, 1 \leqslant i \leqslant d^{\prime}
> $$

------



于是,  只需对协方差矩阵 $\mathbf{X} \mathbf{X}^{\mathrm{T}}$ 进行特征值分解,  将求得的特征值排序: $\lambda_{1} \geqslant \lambda_{2} \geqslant \ldots \geqslant \lambda_{d}$ ,  再取前 $d^{\prime}$ 个特征值对应的特征向量构成 $\mathbf{W}=\left(\boldsymbol{w}_{1}, \boldsymbol{w}_{2}, \ldots, \boldsymbol{w}_{d^{\prime}}\right) \in \mathbb{R}^{d \times d^{\prime}} $ .这就是主成分分析的解.  PCA 算法描述如图 10.5 所示:

![](https://i.loli.net/2019/07/19/5d3194893914e79902.png)

> **注14: 对式 (10.17) 的另外的进一步解释**
>
> 对 $\mathbf{X X}^{\top} \mathbf{W}=\mathbf{W} \mathbf{\Lambda}$ 两边同乘以 $\mathbf{W}^{\top}$ ,  可得:
> $$
> \mathbf{W}^{\top} \mathbf{X} \mathbf{X}^{\top} \mathbf{W}=\mathbf{W}^{\top} \mathbf{W} \mathbf{\Lambda}=\mathbf{\Lambda}
> $$
> 这里, 我们使用了约束条件 $\mathbf{W}^{\top} \mathbf{W}=\mathbf{I}$ .  
>
> 上面的式子的左边与式 (10.16) 的优化目标对应的矩阵相同, 而右边 $\boldsymbol{\Lambda} \in \mathbb{R}^{d^{\prime} \times d^{\prime}}$ 是由 $\mathbf{X} \mathbf{X}^{\mathrm{T}}$ 的 $d^{\prime}$ 个特征值组成的对角阵, 两边同时取矩阵的迹,  得
> $$
> \operatorname{tr}\left(\mathbf{W}^{\top} \mathbf{X} \mathbf{X}^{\top} \mathbf{W}\right)=\operatorname{tr}(\mathbf{\Lambda})=\sum_{i=1}^{d^{\prime}} \lambda_{i}
> $$
> 左边的优化目标相当于最大化 $\sum_{i=1}^{d^{\prime}} \lambda_{i}$ .

------

