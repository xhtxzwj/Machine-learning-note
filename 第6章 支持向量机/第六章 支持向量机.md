# 6.1 间隔与支持向量机

## 6.1.1 问题的提出

给定训练样本集 $D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}, y_{i} \in\{-1,+1\}$ ,  分类学习最基本的想法就是基于训练集 $D$ 在样本空间中找到一个划分超平面,  将不同类别的样本分开.  能将训练样本分开的划分超平面有很多. 如下图.

![](https://i.loli.net/2019/06/03/5cf479a73040845917.png)

可以看出, 粗线条的划分超平面"容忍性"最好.  换句话说,  这个划分超平面所产生的分类结果是最鲁棒的,  对未见示例的泛化能力最强.



## 6.1.2 数学表示

在样本空间中,  划分超平面可通过如下线性方程来描述:
$$
\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b=0 \tag{6.1}
$$
其中,  $\boldsymbol{w}=\left(w_{1} ; w_{2} ; \dots ; w_{d}\right)$ 是法向量,  决定了超平面的方向;  $b$ 为位移项,  决定了超平面与原点之间的距离,  划分超平面可被法向量 $\boldsymbol w$ 和位移 $b$ 确定. 将法向量记为 $(\boldsymbol w, b)$ .

样本空间中任意点 $\boldsymbol x$ 到超平面 $(\boldsymbol w, b)$ 的距离可以写成
$$
r=\frac{\left|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right|}{\|\boldsymbol{w}\|} \tag{6.2}
$$
假设超平面 $(\boldsymbol w, b)$ 能将训练样本正确分类,  即对于 $\left(\boldsymbol{x}_{i}, y_{i}\right) \in D$ ,  若 $y_{i}=+1$ ,  则有 $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b>0$ ;  若 $y_{i}=-1$,  则有 $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b<0$ . 

**令**
$$
\left\{\begin{array}{ll}{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b \geqslant+1,} & {y_{i}=+1} \\ {\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b \leqslant-1,} & {y_{i}=-1}\end{array}\right. \tag{6.3}
$$
如图 (6.2) 

![](https://raw.githubusercontent.com/xhtxzwj/picfiles/master/20190603100556.png)

如图 (6.2) , 距离超平面最近的这几个训练样本点使式 (6.3) 的等号成立, 它们被称为"支持向量机"(support vector) , 两个异类支持向量到超平面的距离之和为:
$$
\gamma=\frac{2}{\|\boldsymbol{w}\|} \tag{6.4}
$$
称为**"间隔"(margin)**

欲找到具有"最大间隔"(maximum margin)的划分超平面,  也就是找到能满足式(6.3)中约束的参数 $\boldsymbol w$ 和 $b$ , 使得 $\gamma$ 最大,  即
$$
\begin{array}{l}{\max _{\boldsymbol{w}, b} \frac{2}{\|\boldsymbol{w}\|}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}\tag{6.5}
$$
最大化间隔, 即最大化 $\|\boldsymbol{w}\|^{-1}$ , 也就是等价于最小化 $\|\boldsymbol{w}\|^{2}$ .  则 (6.5) 可重写为
$$
\begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array} \tag{6.6}
$$
这就是**支持向量机**(support vector machine, 简称SVM)的**基本型**.





# 6.2 对偶问题

## 6.2.1 对偶问题

我们的目的就是根据式 (6.6) 来求得最大间隔划分超平面所对应的模型
$$
f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\tag{6.7}
$$
其中 $\boldsymbol w$ 和 $b$ 是模型参数. (6.6)本身是一个凸二次规划问题, 可以直接求解, 但是有更高效的方法.



对式 (6.6) 使用拉格朗日乘子法可得到其"对偶问题"(dual poblem). 具体来说, 对式 (6.6) 的每条约束添加拉格朗日乘子 $\alpha_{i} \geqslant 0$ , 则该问题的拉格朗日函数可以写成
$$
L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)
$$

> **注1:**  令 $\theta(w)=\max _{\alpha_{i} \geq 0} L(w, b, \alpha)$ , 易验证,当某个约束条件不满足时，例如 $y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)< 1$ ，那么显然有 $\theta(w)=\infty$ (只要令 $\alpha_{i}=\infty$ 即可)。而当所有约束条件都满足时，则有 $\theta(w)=\frac{1}{2}\|\boldsymbol w\|^{2}$，亦即最初要最小化的量。 
>
> 因此，在要求约束条件得到满足的情况下最小化 $\frac{1}{2}\|\boldsymbol w\|^{2}$，实际上等价于直接最小化 $\theta(w)$（当然，这里也有约束条件，就是 $\alpha_{i} \geq 0, i=1, \dots, n$）, 因为如果约束条件没有得到满足， $θ(w) $会等于无穷大，自然不会是我们所要求的最小值 
>
> 具体写出来，目标函数变成了： 
> $$
> \min _{\boldsymbol w, b} \theta(\boldsymbol w)=\min _{\boldsymbol w, b} \max _{\alpha_{i} \geq 0} L(\boldsymbol w, b, \boldsymbol \alpha)=p^{*}
> $$
> 把最小和最大的位置交换一下, 变成:
> $$
> \max _{\alpha_{i} \geq 0} \min _{\boldsymbol w, b} L(\boldsymbol w, b, \boldsymbol \alpha)=d^{*}
> $$



其中, $\boldsymbol{\alpha}=\left(\alpha_{1} ; \alpha_{2} ; \ldots ; \alpha_{m}\right)$ . 令 $L(\boldsymbol{w}, b, \boldsymbol{\alpha})$ 对 $\boldsymbol w$ 和 $b$ 的偏导为零可得
$$
\boldsymbol{w}=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i} \tag{6.9}
$$

$$
0=\sum_{i=1}^{m} \alpha_{i} y_{i}\tag{6.10}
$$

> **注2:**  对 $\boldsymbol w$ 求偏导有:
>
>  $\frac{\partial L}{\partial w}=\frac{1}{2} \times 2 \times \boldsymbol{w}+0-\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}-0=0 \Longrightarrow \boldsymbol{w}=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}$ (1. $\frac {\partial \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}}{\partial \boldsymbol{w}} = \boldsymbol{x}_{i}$ 2. $\frac {\partial \boldsymbol{w}^{\mathrm{T}} \boldsymbol{w}}{\partial \boldsymbol{w}} = 2\boldsymbol{w}$)
>
> $\frac {\partial L}{\partial b}=0+0-0-\sum_{i=1}^{m}\alpha_iy_i=0  \Longrightarrow  \sum_{i=1}^{m}\alpha_iy_i=0$



将式 (6.9)带入式 (6.8) , 即可将 $L(\boldsymbol{w}, b, \boldsymbol{\alpha})$ 中的 $\boldsymbol w$ 和 $b$ 消去, 再考虑式 (6.10) 的约束, 就得到式<font color=red> **(6.6) 的对偶问题** </font>

> **注3:** 
>
> $\begin{aligned}
> L(\boldsymbol{w},b,\boldsymbol{\alpha})  &=\frac{1}{2}\boldsymbol{w}^T\boldsymbol{w}+\sum_{i=1}^m\alpha_i -\sum_{i=1}^m\alpha_iy_i\boldsymbol{w}^T\boldsymbol{x}_i-\sum_{i=1}^m\alpha_iy_ib \\
> &=\frac {1}{2}\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i-\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_
> i -b\sum _{i=1}^m\alpha_iy_i \\
> & = -\frac {1}{2}\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i -b\sum _{i=1}^m\alpha_iy_i
> \end{aligned}$
>
> 又 $0=\sum_{i=1}^{m} \alpha_{i} y_{i}$ , 进一步化简得到
>
> $\begin{aligned}
> L(\boldsymbol{w},b,\boldsymbol{\alpha}) &= -\frac {1}{2}\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i \\
> &=-\frac {1}{2}(\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i)^T(\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i)+\sum _{i=1}^m\alpha_i \\
> &=-\frac {1}{2}\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i \\
> &=\sum _{i=1}^m\alpha_i-\frac {1}{2}\sum_{i=1 }^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j
> \end{aligned}$
>
> 所以,$ \max _{\alpha_{i} \geq 0} \min _{\boldsymbol w, b} L(\boldsymbol w, b, \boldsymbol \alpha)$ , 最后 $L(\boldsymbol w, b, \boldsymbol \alpha)$ 的只有参数 $\alpha_i$ ($\alpha_j$ 一样). 因此 
>
> $ \max _{\alpha_{i} \geq 0} \min _{\boldsymbol w, b} L(\boldsymbol w, b, \boldsymbol \alpha)=\max _{\boldsymbol{\alpha}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}$



<font color=red> **(6.6) 的对偶问题** </font>
$$
\max _{\boldsymbol{\alpha}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j} \tag{6.11}
$$

$$
s.t. \begin{array}{l}{\sum\limits_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}
$$

## 6.2.2 KKT条件

解出 $\boldsymbol \alpha$ 后, 求出 $\boldsymbol w$ 和$b$ 即可得到模型
$$
\begin{aligned} f(\boldsymbol{x}) &=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}+b \end{aligned} \tag{6.12}
$$
从对偶问题 (6.11)解出的 $\alpha_{i}$ 是式 (6.8) 中的拉格朗日乘子, 它恰对应着训练样本$\left(\boldsymbol{x}_{i}, y_{i}\right)$ . 注意到式 (6.6) 中有不等式约束,  因此上述过程需满足 KKT(Karush-Kuhn-Tucker) 条件, 即要求 
$$
\left\{\begin{array}{l}{\alpha_{i} \geqslant 0} \\ {y_{i} f\left(\boldsymbol{x}_{i}\right)-1 \geqslant 0} \\ {\alpha_{i}\left(y_{i} f\left(\boldsymbol{x}_{i}\right)-1\right)=0}\end{array}\right.
$$

> **注4:** 关于拉格朗日对偶性以及KKT条件, 详细知识参见统计学习方法附录C, 别问为什么是这样的, 再问就是"易得"



于是,  对任意训练样本 $\left(\boldsymbol{x}_{i}, y_{i}\right)$,  总有 $\alpha_{i}=0$ 或 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1$ .若 $\alpha_{i}=0$ ,  则该样本将不会在式 (6.12) 的求和中出现,  也就不会对 $f(\boldsymbol x)$ 有任何影响;  若 $\alpha_{i}>0$,  则必有 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1$ ,  所对应的样本点位于最大间隔边界上,  是一个支持向量.  这显示出支持向量机的一个重要性质:训练完成后,  大部分的训练样本都不需保留,  最终模型仅与支持向量有关 .

## 6.2.3 SMO算法

暂放



# 6.3 核函数

## 6.3.1 线性不可分的情况

前面的都是假设训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类.然而在现实任务中,  原始样本空间内也许并不存在一个能正确划分两类样本的超平面.  如图 6.3 中的" 异或 " 问题就不是线性可分的.

![](https://raw.githubusercontent.com/xhtxzwj/picfiles/master/20190603201624.png)

对这样的问题,  **可将样本从原始空间映射到一个更高维的特征空间,  使得样本在这个特征空间内线性可分.** 

如在图 6.3 中,  若将原始的二维空间映射到一个合适的三维空间,  就能找到一个合适的划分超平面.

同时,  **如果原始空间是有限维,  即属性数有限,  那么一定存在一个高维特征空间使样本可分** 



## 6.3.2 数学表示

令 $\phi(\boldsymbol{x})$ 表示将 $\boldsymbol x$ 映射后的特征向量,  于是, 在特征空间中划分超平面所对应的模型可表示为:
$$
f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \phi(\boldsymbol{x})+b \tag{6.19}
$$
其中, $\boldsymbol w$ 和 $b$ 是模型参数. 类似于式 (6.6), 有
$$
\begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \phi\left(\boldsymbol{x}_{i}\right)+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array} \tag{6.20}
$$
其对偶问题是
$$
\max _{\boldsymbol \alpha} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right) \tag{6.21}
$$

$$
s.t. \begin{array}{l}{\sum\limits_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}
$$

$\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)$ 表示样本 $\boldsymbol{x}_{i}$ 与 $\boldsymbol{x}_{j}$ 映射到特征空间之后的内积. 计算较为为难.  为避开,  设想这样一个函数:
$$
\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\langle\phi\left(\boldsymbol{x}_{i}\right), \phi\left(\boldsymbol{x}_{j}\right)\right\rangle=\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right) \tag{6.22}
$$
即  $\boldsymbol{x}_{i}$ 与 $\boldsymbol{x}_{j}$ 在特征空间的内积等于它们在原始样本空间中通过函数 $\kappa(\cdot, \cdot)$ 计算的结果.

于是, 式 (6.21) 可以重写为:
$$
\max _{\boldsymbol{\alpha}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \tag{6.23}
$$

$$
s.t. \begin{array}{l}{\sum\limits_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}
$$

求解后即可得到
$$
\begin{aligned} f(\boldsymbol{x}) &=\boldsymbol{w}^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \kappa\left(\boldsymbol{x}, \boldsymbol{x}_{i}\right)+b \end{aligned} \tag{6.24}
$$
这里的函数 $\kappa(\cdot, \cdot)$ 就是**"核函数"**(kernel function)

式 (6.24) 显示出模型最优解可通过训练样本的函数展开, 这一展开式亦称 "支持向量展式"



## 6.3.3 有关核函数的定理

### 1 定理6.1 (核函数)

定理6.1 (核函数)  令 $\mathcal{X}$ 为输入空间,   $\kappa(\cdot, \cdot)$ 是定义在 $\mathcal{X} \times \mathcal{X}$ 上的对称函数,  则 $k$ 是核函数当且仅当对于任意数据 $D=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\}$ ,  "核矩阵'' (kernel matrix) $\mathbf{K}$ 总是本正定的:
$$
\mathbf{K}=\left[\begin{array}{ccccc}{\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{1}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{j}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{m}\right)} \\ {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{1}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{m}\right)} \\ {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{1}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{j}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{m}\right)}\end{array}\right]
$$
定理 6.1 表明,  **只要一个对称函数所对应的核矩阵半正定,  它就能作为核函数使用.**  事实上,  **对于一个半正定核矩阵,  总能找到一个与之对应的映射 $\phi**$ .  换言之,  任何一个核函数都隐式地定义了一个称为"再生核希尔伯特空间" (Reproducing Kernel Hilbert Space，简称 RKHS) 的特征空间. 

### 2 常用的核函数及核函数的相关组合

表 6.1 列出了集中常用的核函数

![](https://raw.githubusercontent.com/xhtxzwj/picfiles/master/20190603210644.png)

此为, 还可以通过函数组合得到相关的核函数. 

- 若 $k_{1}$ 和 $k_{2}$ 为核函数,  则对于任意正数 $\gamma_{1}, \gamma_{2}$ ,  其线性组合
  $$
  \gamma_{1} \kappa_{1}+\gamma_{2} \kappa_{2} \tag{6.25}
  $$
  也是核函数

- 若 $k_{1}$ 和 $k_{2}$ 为核函数,  则核函数的直积
  $$
  \kappa_{1} \otimes \kappa_{2}(\boldsymbol x, \boldsymbol z)=\kappa_{1}(\boldsymbol x, \boldsymbol z) \kappa_{2}(\boldsymbol x, \boldsymbol z) \tag{6.26}
  $$
  也是核函数

- 若 $k_{1}$ 为核函数,  则对于任意函数 $g(\boldsymbol{x})$ ,
  $$
  \kappa(\boldsymbol{x}, \boldsymbol{z})=g(\boldsymbol{x}) \kappa_{1}(\boldsymbol{x}, \boldsymbol{z}) g(\boldsymbol{z})
  $$
  也是函数



# 6.4 软间隔与正则化

## 6.4.1 软间隔的概念

前面我们假定训练样本在**样本空间**或**特征空间**中是**线性可分**的,  即存在一个超平面能将不同类的样本完全划分开.  然而,  在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分;  即使恰好找到了 某个核函数使训练集在特征空间中线性可分,  也很难断定这个貌似线性可分的结果不是由于**过拟合**所造成的.  

缓解该问题的一个办法是<font color=red>**允许支持向量机在一些样本上出错**</font>.为此,  要引入"**软间隔**" (soft margin)的概念,  如图6.4所示.

![](https://i.loli.net/2019/06/04/5cf5d97f9bd8d70600.png)



## 6.4.2 数学表示

前面介绍的支持向量机形式是**要求所有样本均满足约束 (6.3)**,  即**所有样本都必须划分正确**,  这称为" **硬间隔**" (hard margin),  而**软间隔则是允许某些样本不满足约束**
$$
y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1\tag{6.28}
$$
在**最大化间隔的同时,  不满足约束的样本应尽可能的少**.  则优化目标可以写为
$$
\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell_{0 / 1}\left(y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)-1\right)\tag{6.29}
$$
其中 $C>0$ 是一个常数,  $\ell_{0 / 1}$ 是 "0/1损失函数"
$$
\ell_{0 / 1}(z)=\left\{\begin{array}{ll}{1,} & {\text { if } z<0} \\ {0,} & {\text { otherwise }}\end{array}\right. \tag{6.30}
$$
<font color=green>**显然,  当 $C$ 为无穷大时, 式 (6.29) 迫使所有样本均满足约束 (6.28) ,  于是式 (6.29) 等价于 式 (6.6) ;  当 $C$ 取有限值时,  式 (6.29) 允许一些样本不满足约束.**</font>



然而,  $\ell_{0 / 1}$ 非凸、非连续,  数学性质不太好,  使得式 (6.29) 不易直接求解. 人们通常用其他一些函数来代替 $\ell_{0 / 1}$,   称为"**替代损失**" (surrogate loss).  替代损失函数一般具有较好的数学性质,  如它们通常是凸的连续函数且是  $\ell_{0 / 1}$ 的上界.  图 6.5 给出了三种常用的替代损失函数: 

- hinge 损失:  $\ell_{\text {hinge}}(z)=\max (0,1-z)\tag{6.31}$

- 指数损失(exponential loss):  $\ell_{e x p}(z)=\exp (-z)\tag{6.32}$

- 对率损失(logistic loss):   $\ell_{\exp }(z)=\exp (-z) \tag{6.33}$

![](https://i.loli.net/2019/06/04/5cf6202aa024751841.png)

若采用hinge损失,  则式 (6.29) 变成
$$
\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \max \left(0,1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right) \tag{6.34}
$$

> 注5:  同6.29一样的思路来理解



接着,  引入"**松弛变量**"(slack variable) $\xi_{i} \geqslant 0$ ,  可将式 (6.34) 重写为
$$
\min _{\boldsymbol{w}, b, \xi_{i}} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i} \tag{6.35}
$$

$$
s.t. \quad y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1-\xi_{i}
$$

​                                                                                  $\xi_{i} \geqslant 0, i=1,2, \ldots, m$

这就是常用的<font color=red>**"软间隔支持向量机"**</font>



> 注6: 式(6.35)是式(6.34)的上限，最小化式(6.35)的同时也会最化小式(6.34)，这是因为:
>
> 由式(6.35)的约束条件 $y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geqslant 1-\xi_{i}$ 可得 $\xi_{i} \geqslant 1-y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)$，再加上约束条件 $\xi_{i} \geqslant 0$，即 $\xi_{i} \geqslant \max \left(0,1-y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right)$ ，因此式(6.35)是式(6.34)的上限 



显然,  式 (6.35) 中每个意昂本都有一个对应的松弛变量,  用以表征该样本不满足约束 (6.28) 的程度.  

与式 (6.6) 相似,  这还是一个二次规划问题.  于是,  类似于式 (6.8) ,  通过拉格朗日乘子法可得到式 (6.35) 的拉格朗日函数
$$
\begin{aligned} L(\boldsymbol{w}, b, \boldsymbol{\alpha}, \boldsymbol{\xi}, \boldsymbol{\mu})=& \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i} \\ &+\sum_{i=1}^{m} \alpha_{i}\left(1-\xi_{i}-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i} \end{aligned} \tag{6.36}
$$
其中,  $\alpha_{i} \geqslant 0, \mu_{i} \geqslant 0$ 是拉格朗日乘子.

> ------
>
> 注7:  **拉格朗日基本形式**为
> $$
> \min _{x \in \mathbf{R}^{n}} f(x)
> $$
>
> $$
> s.t. \quad c_{i}(x) \leqslant 0, \quad i=1,2, \cdots, k
> $$
>
> $$
> h_{j}(x)=0, \quad j=1,2, \cdots, l
> $$
>
> $$
> L(x, \alpha, \beta)=f(x)+\sum_{i=1}^{k} \alpha_{i} c_{i}(x)+\sum_{j=1}^{l} \beta_{j} h_{j}(x)
> $$
>
> 引入拉格朗日函数有
> $$
> L(x, \alpha, \beta)=f(x)+\sum_{i=1}^{k} \alpha_{i} c_{i}(x)+\sum_{j=1}^{l} \beta_{j} h_{j}(x)
> $$

------

令 $L(\boldsymbol{w}, b, \boldsymbol{\alpha}, \boldsymbol{\xi}, \boldsymbol{\mu})$ 对 $\boldsymbol w, b, \xi_{i}$ 的偏导为零可得
$$
\boldsymbol{w}=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i} \tag{6.37}
$$

$$
0=\sum_{i=1}^{m} \alpha_{i} y_{i}\tag{6.38}
$$

$$
C=\alpha_{i}+\mu_{i}\tag{6.39}
$$

将式 (6.37)-(6.39) 代入式 (6.36) 即可得到式 (6.35) 的<font color=red>**对偶问题**</font>

------


$$
\max _{\boldsymbol{\alpha}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j} \tag{6.40}
$$

$$
s.t. \sum_{i=1}^{m} \alpha_{i} y_{i}=0
$$

​                                                                                         $0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \ldots, m$

------

软间隔的对偶问题式 (6.40) 与硬间隔下的对偶问题 (6.11) 对比可看出,  两者唯一的差别就再与对偶变量的约束不同: 前者是 $0 \leqslant \alpha_{i} \leqslant C$ ,  后者是 $0 \leqslant \alpha_{i}$ . 于是，可采用 6.2节中同样的算法求解式 (6.40);  在引入**核函数**后能得到与式 (6.24) 同样的**支持向量展式**. 



类似于式 (6.13),  对软间隔支持向量机,  KKT条件要求
$$
\left\{\begin{array}{l}{\alpha_{i} \geqslant 0, \quad \mu_{i} \geqslant 0} \\ {y_{i} f\left(\boldsymbol{x}_{i}\right)-1+\xi_{i} \geqslant 0} \\ {\alpha_{i}\left(y_{i} f\left(\boldsymbol{x}_{i}\right)-1+\xi_{i}\right)=0} \\ {\xi_{i} \geqslant 0, \quad \mu_{i} \xi_{i}=0}\end{array}\right.\tag{6.41}
$$
对任意训练样本 $\left(\boldsymbol{x}_{i}, y_{i}\right)$ ,  总有 $\alpha_{i}=0$ 或 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1-\xi_{i}$ .   若 $\alpha_{i}=0$ ,  则该样本不会对 $f(\boldsymbol x)$ 有任何影响;  若  $\alpha_{i}>0$ ,  则必有 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1-\xi_{i}$ ,  即该样本是支持向量:   由式 (6.39)可知, 若 $\alpha_{i}<C$ ,  则 $\mu_{i}>0$ ,  进而有 $\xi_{i}=0$ ,  即该样本恰在最大间隔边界上;  若 $\alpha_{i}=C$ ,  则有 $\mu_{i}=0$ ,  此时若 $\xi_{i} \leqslant 1$ , 则该样本落在最大间隔内部, 若 $\xi_{i}>1$ , 则该样本被错误分类.   

由此可看出,  软间隔支持向量机的最终模型**仅与支持向量有关**,  即通过采用hinge损失函数仍**保持了稀疏性**. 